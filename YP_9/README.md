Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. 
Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.
Постройте модель со значением метрики качества F1 не меньше 0.75. 

1  Описание проекта
Для интернет-магазина «Викишоп» необходим инструмент, позволяющий определять "токсичные" комментарии среди отредактированных пользователями комментариев товаров.

1.1  Цель
Построить модель классификации комментариев пользователей на "токсичные" и "позитивные".

1.2  Задачи
Обработать предоставленную выборку
Подготоить незаисимые признаки для моделей классификации
Построить модели классификации тектовой информации
Добиться значения метрики качества F1 не меньше 0.75.
1.3  Описание данных
Данные лежат в файле toxic_comments.csv.
Целевая переменная находится в столбце toxic.

Целью исследования было подготовить модель кластеризации комментариев (текстов) на токсичные и нетоксичные. Модель должна была быть разработана на основании заранее размеченного корпуса текстов (предостален в csv файле). Показателем качества модели требовалось принять F1-меру, которая должна быть не меньше 0.75 на тестовых данных. При выборе модели важна только точность предсказания. В процесе исследования:

Загружен корпус размеченных текстов
Исследованы загруженные данные
Выполнен EDA, включая:
проверку корректности назначенных типов данных (ничего не меняли)
удаление неинформативных столбцов ('Unnamed:0')
изучение текстовой информации (тексты содержат излишне повторяющиеся символы, "мусорные" символы, строчные и прописные символы, пробелы)
проверка сбалансированности целевой переменной (переменная не сбалансирована)
Исключен дисбаланас целевой переменной за счет удаления части "нетоксичных" строк.
Проведена подготовка данных к машинному обучению, тексты отчищены:
от повторяющих более 3-х раз подряд символов
от мусорных символов (;$& и т.д.)
от лишних пробелов
от стоп-слов
Тексты лемматизированы
Для текстов посчитаны матрицы TF-IDF (статистическая мера, используемая для оценки важности слова в контексте документа)
Размерность матрицы снижена за счет удаления редких слов (менее 3 вхождений)
Для текстов локально посчитаны и загружены через google-диск эмбендинги предъобученной модели BERT
Подготовлены функции, автоматизирующие:
предъобратотку текстов
расчет, считывание и запись эмбендингов
перебор гиперпараметров на основе метода GridSearch и кросс-валидации
фиксацию результатоввычислений в отдельных ДФ
Проведено машинное обучение, использованы модели:
LogisticRegression
RandomForestClassifier
HistGradientBoostingClassifie
KNeighborsClassifier
Подобраны гиперпараметры для указанных моделей, оценена метрика качества, полученная на обучающей выборке, модель с оптимальными гиперпараметрами проверены на тестовой выборке.
По результатам сравнение результатов, установлено, что лучшей моделью является LightGBM c F1 на тестовой выборке 0.78
Результаты визуализированы ROC_AUC кривой
